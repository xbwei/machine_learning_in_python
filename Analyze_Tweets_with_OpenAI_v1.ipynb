{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dc3883c",
   "metadata": {},
   "source": [
    "# Analyze Twitter Data with OpenAI V1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e679fb6",
   "metadata": {},
   "source": [
    "This notebook assumes that your Tweets were collected with Twitter API V1, or the Tweets are orgianzied as:\n",
    "```\n",
    "{\n",
    "id:123,\n",
    "text:'abc',\n",
    "...\n",
    "}\n",
    "\n",
    "```\n",
    "If you Tweets were collected with Twitter API V2 or organized in a different foramt, please use the code for [V2](https://github.com/xbwei/machine_learning_in_python/tree/master)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcc5219",
   "metadata": {},
   "source": [
    "## Install Python libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756e9eb5",
   "metadata": {},
   "source": [
    "We need the [pymongo](https://pypi.org/project/pymongo/) to manage the MongoDB database, and [openai](https://github.com/openai/openai-python) to call the OpenAI APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e207189b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymongo\n",
      "  Downloading pymongo-4.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
      "Collecting dnspython<3.0.0,>=1.16.0 (from pymongo)\n",
      "  Downloading dnspython-2.4.2-py3-none-any.whl.metadata (4.9 kB)\n",
      "Downloading pymongo-4.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (677 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m677.1/677.1 kB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dnspython-2.4.2-py3-none-any.whl (300 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.4/300.4 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: dnspython, pymongo\n",
      "Successfully installed dnspython-2.4.2 pymongo-4.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0779bc84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Downloading openai-1.2.3-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting anyio<4,>=3.5.0 (from openai)\n",
      "  Downloading anyio-3.7.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Downloading distro-1.8.0-py3-none-any.whl (20 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from openai)\n",
      "  Downloading httpx-0.25.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting pydantic<3,>=1.9.0 (from openai)\n",
      "  Downloading pydantic-2.4.2-py3-none-any.whl.metadata (158 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.6/158.6 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from openai) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from openai) (4.8.0)\n",
      "Requirement already satisfied: idna>=2.8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anyio<4,>=3.5.0->openai) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anyio<4,>=3.5.0->openai) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anyio<4,>=3.5.0->openai) (1.1.3)\n",
      "Requirement already satisfied: certifi in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
      "Collecting httpcore (from httpx<1,>=0.23.0->openai)\n",
      "  Downloading httpcore-1.0.2-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic<3,>=1.9.0->openai)\n",
      "  Downloading annotated_types-0.6.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pydantic-core==2.10.1 (from pydantic<3,>=1.9.0->openai)\n",
      "  Downloading pydantic_core-2.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore->httpx<1,>=0.23.0->openai)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading openai-1.2.3-py3-none-any.whl (220 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.3/220.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading anyio-3.7.1-py3-none-any.whl (80 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.9/80.9 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpx-0.25.1-py3-none-any.whl (75 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.4.2-py3-none-any.whl (395 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m395.8/395.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pydantic-core, h11, distro, anyio, annotated-types, pydantic, httpcore, httpx, openai\n",
      "  Attempting uninstall: anyio\n",
      "    Found existing installation: anyio 4.0.0\n",
      "    Uninstalling anyio-4.0.0:\n",
      "      Successfully uninstalled anyio-4.0.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sparkmagic 0.21.0 requires pandas<2.0.0,>=0.17.1, but you have pandas 2.1.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed annotated-types-0.6.0 anyio-3.7.1 distro-1.8.0 h11-0.14.0 httpcore-1.0.2 httpx-0.25.1 openai-1.2.3 pydantic-2.4.2 pydantic-core-2.10.1\n"
     ]
    }
   ],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12530bd4",
   "metadata": {},
   "source": [
    "## Import Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7d944e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "import json\n",
    "from pprint import pprint\n",
    "import configparser\n",
    "from tqdm.auto import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17f430f",
   "metadata": {},
   "source": [
    "## Load the authorization info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52260a80",
   "metadata": {},
   "source": [
    "Save the database connection info and API key in a config.ini file and use the configparse to load the authorization info.\n",
    "\n",
    "The config.ini file should look like:\n",
    "``` \n",
    "[myopenai]\n",
    "openai_api = <your openai API>\n",
    "\n",
    "[mymongo]\n",
    "connection = <your monogdb connection>\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d56a489b",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser(interpolation=None)\n",
    "config.read('config.ini')\n",
    "\n",
    "openai_api_key   = config['myopenai']['openai_api']\n",
    "\n",
    "mongod_connect = config['mymongo']['connection']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c50e000",
   "metadata": {},
   "source": [
    "## Connect to the MongoDB cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d694834",
   "metadata": {},
   "source": [
    "We will connect to the MongoDB database that contains the tweet data. You need to change the database name and collection name to match your settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9102f89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient(mongod_connect)\n",
    "db = client.tweet # use or create a database named tweet\n",
    "tweet_collection = db.gun_va #use or create a collection named gun_va\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e6941d",
   "metadata": {},
   "source": [
    "## Extract Twitter Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0099a8b",
   "metadata": {},
   "source": [
    "Search the Tweets you are intrested.\n",
    "You can use [MongoDB Compass](https://www.mongodb.com/try/download/compass) to help you write the queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8420470a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The following code is generated in MongoDB Compass to find the top 100 tweets \n",
    "with a key word of 'shooting', ordered by the favorite count\n",
    "'''\n",
    "filter={\n",
    "    '$text': {\n",
    "        '$search': 'shooting'\n",
    "    }\n",
    "}\n",
    "project={\n",
    "    'id': 1, \n",
    "    'text': 1\n",
    "}\n",
    "sort=list({\n",
    "    'favorite_count': -1\n",
    "}.items())\n",
    "limit=100\n",
    "result = client['tweet']['gun_va'].find(\n",
    "  filter=filter,\n",
    "  projection=project,\n",
    "  sort=sort,\n",
    "  limit=limit\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76637a42",
   "metadata": {},
   "source": [
    "Save the extracted Tweets into the ```tweet_data``` list. Remove URLs and new lines to save the tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f7dbaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_data = []\n",
    "url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "for tweet in result:\n",
    "    text_without_urls = re.sub(url_pattern, '', tweet['text'])\n",
    "    tweet_data.append({'tweet_id':tweet['id'],'tweet_text':text_without_urls.replace('\\n','')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26b9f24a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets:  73\n"
     ]
    }
   ],
   "source": [
    "print('Number of tweets: ',len(tweet_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0764c0",
   "metadata": {},
   "source": [
    "## Set up OpenAI API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641989e5",
   "metadata": {},
   "source": [
    "Load the OpenAI API key and set the API parameters. \n",
    "- Model type: use ```gpt-3.5-turbo``` by default, and you choose any [avaiabel models](https://platform.openai.com/docs/models/overview).\n",
    "- Token estimate: 100 tokens ~= 75 words in English. Total token usage = tokens in the prompot + tokens in the completion. You can get a more accurate estimate at [Tokenier](https://platform.openai.com/tokenizer).\n",
    "- Temperature: use default value 0. Lower temperature result in more consistent outputs, while higher values generate more diverse and creative results\n",
    "\n",
    "A help funciton, ```openai_help```, is created to pass the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1235902",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=openai_api_key)\n",
    "model=\"gpt-3.5-turbo\"\n",
    "temperature=0\n",
    "\n",
    "\n",
    "\n",
    "def openai_help(prompt, model=model, temperature =temperature ):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature\n",
    "\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1564c1",
   "metadata": {},
   "source": [
    "## Sentiment anlysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f10f947",
   "metadata": {},
   "source": [
    "Analyze the sentiment of each tweet and save the result to the MongoDB database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "271cf78d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bc5beeec6554078bbfb3918b6c5d722",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for tweet in tqdm(tweet_data):\n",
    "  \n",
    "    prompt = f\"\"\"\n",
    "    What is the sentiment of the following tweet, \n",
    "    tweet text: {tweet['tweet_text']}\n",
    "    return  the result with one word as positive, neutral,or negative\n",
    " \n",
    "    \"\"\"\n",
    "#     print(prompt)\n",
    "    try:\n",
    "        sentiment_result =openai_help(prompt)\n",
    "    #     print(sentiment_result)\n",
    "\n",
    "        tweet_collection.update_one(\n",
    "            {'id':tweet['tweet_id']},\n",
    "            {\"$set\":{'sentiment':sentiment_result}}\n",
    "        )\n",
    "    except:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f57010",
   "metadata": {},
   "source": [
    "## Language translation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eae92b5",
   "metadata": {},
   "source": [
    "Translate each tweet into a different language, and save the result to the MongoDB database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55bfe879",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c571b77cd4c4f768d2104a942f1a0f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for tweet in tqdm(tweet_data):\n",
    "  \n",
    "    prompt = f\"\"\"\n",
    "    Translate the follwoing tweet into Chinese\n",
    "    tweet text: {tweet['tweet_text']}\n",
    " \n",
    "    \"\"\"\n",
    "#     print(prompt)\n",
    "    try:\n",
    "        translate_result =openai_help(prompt)\n",
    "#         print(translate_result)\n",
    "\n",
    "        tweet_collection.update_one(\n",
    "            {'id':tweet['tweet_id']},\n",
    "            {\"$set\":{'translate':translate_result}}\n",
    "        )\n",
    "    except:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d4ae49",
   "metadata": {},
   "source": [
    "## Identify emotions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c81c2f",
   "metadata": {},
   "source": [
    "Identify whether a tweet expresses anger, and save the result to the MongoDB database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ee5457f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b152cea56cf4d59828a7a6decf7b609",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for tweet in tqdm(tweet_data):\n",
    "  \n",
    "    prompt = f\"\"\"\n",
    "    Does the following tweet express anger?\n",
    "    Provide the result as eitehr True or False.\n",
    "    tweet text: {tweet['tweet_text']}\n",
    " \n",
    "    \"\"\"\n",
    "#     print(prompt)\n",
    "    try:\n",
    "        emotion_result =openai_help(prompt)\n",
    "    #     print(emotion_result)\n",
    "\n",
    "        tweet_collection.update_one(\n",
    "                {'id':tweet['tweet_id']},\n",
    "                {\"$set\":{'anger':emotion_result}}\n",
    "            )\n",
    "    except:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e8ffd0",
   "metadata": {},
   "source": [
    "## Extract entities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f3066f",
   "metadata": {},
   "source": [
    "Extract person and organzation names from each tweet and save the result to the MongoDB database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c72c10d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d400d82792a4a5cafa707b48896c657",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for tweet in tqdm(tweet_data):\n",
    "  \n",
    "    prompt = f\"\"\"\n",
    "    Identify persons or organzations from the following tweet,\n",
    "    tweet text: {tweet['tweet_text']},\n",
    "    format the response as a JSON document with person and organzation as the keys.\n",
    "    If the information is not presented, use \"unknown\".\n",
    "    \"\"\"\n",
    "#     print(prompt)\n",
    "    try:\n",
    "        extract_result =openai_help(prompt)\n",
    "#         print(extract_result)\n",
    "\n",
    "        tweet_collection.update_one(\n",
    "                {'id':tweet['tweet_id']},\n",
    "                {\"$set\":{'extracted_item':json.loads(extract_result)}}\n",
    "                )\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364887f5",
   "metadata": {},
   "source": [
    "## Summarize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84fff0d",
   "metadata": {},
   "source": [
    "Summarize the tweet texts with a specific focus, and save the result to the MongoDB database.\n",
    "By default, 50 tweets are analyzed in each batch. You can change the batch size based on the model you use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab3b7dd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define the batch size\n",
    "batch_size = 50\n",
    "\n",
    "start_index = 0\n",
    "\n",
    "\n",
    "while start_index < len(tweet_data):\n",
    "    batch = tweet_data[start_index:start_index + batch_size]\n",
    "\n",
    "    tweet_id_list =[]\n",
    "    tweet_text_summary =''\n",
    "    \n",
    "    for tweet in batch:\n",
    "        tweet_id_list.append(tweet['tweet_id'])\n",
    "        tweet_text_summary = tweet_text_summary+'.'+tweet['tweet_text']\n",
    "        \n",
    "    prompt = f\"\"\"\n",
    "    Summarize the following tweets in at most 50 words, \n",
    "    and focusing why people oppose gun control\n",
    "    tweet text: {tweet_text_summary,}\n",
    " \n",
    "    \"\"\"\n",
    "#     print(prompt)\n",
    "    try:\n",
    "        summary_result =openai_help(prompt)\n",
    "        \n",
    "        tweet_summary = db.tweet_summary #use or create a collection named gun_va\n",
    "        tweet_summary.insert_one({'id_list':tweet_id_list,\n",
    "                            'tweet_text_summary':summary_result})\n",
    "        print(summary_result,'\\n')\n",
    "    except:\n",
    "        pass\n",
    "    start_index += batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ea5f13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
